# ProgRAG 项目复现指南

## 项目简介

**ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs**

这是一个发表在 AAAI 2026 的论文实现，旨在解决知识图谱问答中的幻觉问题。该方法采用渐进式检索和推理策略，通过多步骤的关系检索、三元组检索和评分来逐步构建答案推理路径。

### 核心特点

1. **渐进式检索**：逐步检索关系和三元组，而不是一次性检索所有候选
2. **多模型融合**：结合 GNN 和 MPNet 两种检索器，提高检索质量
3. **问题分解**：将复杂问题分解为多个子问题，逐步解决
4. **抗幻觉机制**：通过多步骤验证和路径评分减少错误答案

---

## 环境配置

### 步骤 1: 创建 Conda 环境

```bash
# 创建并激活新的 conda 环境
conda create -n ProgRAG python=3.8 -y
conda activate ProgRAG
```

### 步骤 2: 安装 PyTorch (CUDA 11.8)

```bash
conda install pytorch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 pytorch-cuda=11.8 -c pytorch -c nvidia
```

### 步骤 3: 安装 PyTorch Geometric 及相关包

```bash
# 安装 PyTorch Geometric 的依赖包
pip install torch-scatter==latest+cu118 torch-sparse==latest+cu118 torch-cluster==latest+cu118 torch-spline-conv==latest+cu118 -f https://data.pyg.org/whl/torch-2.2.1+cu118.html

# 安装 PyTorch Geometric
pip install torch-geometric==2.3.0
```

### 步骤 4: 安装其他依赖

```bash
conda install ninja easydict pyyaml -c conda-forge
```

### 步骤 5: 安装 Hugging Face 相关包

```bash
pip install transformers==4.46.3 tokenizers==0.20.0 huggingface_hub==0.36.0 safetensors==0.5.3
```

### 步骤 6: 安装 Sentence Transformers 和 Datasets

```bash
pip install "sentence-transformers[train]==3.0.1" datasets==2.14.7
```

### 额外依赖

根据代码分析，还需要安装：
```bash
pip install networkx scipy openai tqdm
```

---

## 数据准备

### 方法 1: 直接下载（推荐）

从 Google Drive 下载预处理好的数据集和模型检查点：

🔗 **下载链接**: https://drive.google.com/drive/folders/1BVvQRNTaLdONEeFauZfxPYQXQSpCVuNm?usp=drive_link

下载后需要包含以下内容：

1. **数据集文件**（放在 `/data/graphs/` 目录）：
   - `total_graph_webqsp.jsonl`
   - `total_graph_cwq.jsonl`
   - `webqsp_triple2id.pkl`
   - `cwq_triple2id.pickle`
   - `webqsp_topic_graph.pickle`
   - `cwq_topic_graph_updated.pickle`

2. **模型检查点**（放在 `./ckpt/` 目录）：
   - `./ckpt/GNN/webqsp/GNN.pth`
   - `./ckpt/GNN/cwq/GNN.pth`
   - `./ckpt/mpnet/webqsp.mdl`
   - `./ckpt/mpnet/cwq.mdl`
   - `./ckpt/sbert/` (关系检索器模型)

### 方法 2: 自行预处理（适用于已有原始数据）

如果需要从头开始预处理数据：

export HF_HOME=./hf_cache
export HF_ENDPOINT=https://hf-mirror.com
export HUGGINGFACE_HUB_CACHE=$HF_HOME
设置镜像源以及下载的位置
```bash
# 1. 预处理图谱数据
python3 graph_preprocess.py

会生成total_graph_cwq.jsonl
脚本会分别从 rmanluo/RoG-cwq 的 train/validation/test 切分里，把 graph 字段里的所有三元组收集起来去重，然后一次性写成一个 JSON 列表存进这个文件，表示全量知识图谱的边集合。
cwq_topic_graph.pickle
脚本遍历三个切分中的每条样本，对 q_entity 里的每个主题实体收集其对应子图中的三元组（同样做并集去重），再依赖现有的 cwq_triple2id（脚本会在 data/graphs/cwq_triple2id.pkl|.pickle 或 data/cwq/cwq_triple2id.json 中查找）把三元组映射成 ID，最终序列化成一个 topic -> [triple_id, ...] 的字典并写入该 pickle 文件。

第二步需要用到转化的pickle文件，使用命令转化

python - <<'PY'
import os
from pathlib import Path
import pickle
from datasets import load_dataset
from tqdm import tqdm

# --- 设置离线模式 + 路径 ---
os.environ["HF_DATASETS_OFFLINE"] = "1"   # 若缓存已下载，可保留；否则注释掉以允许联网
os.environ["HF_HUB_OFFLINE"] = "1"

DATASET = "cwq"
SPLITS = ["train", "validation", "test"]
OUT_PATH = Path("data/graphs") / f"{DATASET}_triple2id.pickle"

triples = set()

for split in SPLITS:
    print(f"[INFO] 加载 {split} split …")
    ds = load_dataset(f"rmanluo/RoG-{DATASET}", split=split)
    for item in tqdm(ds, desc=f"收集三元组({split})"):
        for triple in item["graph"]:
            triples.add(tuple(triple))
    print(f"[INFO] {split} 累计去重三元组数: {len(triples):,}")

print("[INFO] 排序并编号…")
triples = sorted(triples)
triple2id = {triple: idx for idx, triple in enumerate(triples)}
print(f"[INFO] 完成编号，共 {len(triple2id):,} 条")

OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
with open(OUT_PATH, "wb") as f:
    pickle.dump(triple2id, f)
print(f"[DONE] 保存到 {OUT_PATH.resolve()}")
PY

cwq_triple2id.pickle 生成且为字典结构
之后再执行第二步

python - <<'PY'
import graph_preprocess as gp
gp.make_topic2graph('cwq')
PY
生成
data/graphs/cwq_topic_graph.pickle


# 2. 生成 GNN 嵌入
python3 GNN/get_emb.py
```
会下载gte-large-en-v1.5模型
生成
关系嵌入文件 (data/cwq/emb/relation.pth)
映射文件 (entity2id.pkl, rel2id.pkl)
**注意**: 数据预处理需要原始数据集（如 RoG-webqsp 和 RoG-cwq），这些数据集会通过 Hugging Face datasets 库自动下载。

---

## 模型训练（可选）

如果下载的检查点不可用，可以自行训练模型：

### 1. 训练 GNN 模型

```bash
python3 GNN/gnn_train.py
```
### 测试gnn
export CC=/usr/bin/gcc-11
export CXX=/usr/bin/g++-11

```bash
python test_gnn_model.py cwq cuda:2
```

### 2. 训练 MPNet 模型

根据数据集选择对应的训练脚本：

```bash
# 对于 WebQSP 数据集
bash MPNet/webqsp.sh

# 对于 CWQ 数据集
bash MPNet/cwq.sh
```

### 3. 训练关系检索器（CrossEncoder）

```bash
python3 finetuning_crossencoder.py
```

---

## 运行推理

### 基本用法

```bash
python main.py --dataset [DATASET_NAME]
```

**支持的数据集**:
- `webqsp`: WebQuestionsSP 数据集
- `cwq`: ComplexWebQuestions 数据集

### 示例命令

```bash
# 在 WebQSP 测试集上运行
python main.py --dataset webqsp

# 在 CWQ 测试集上运行
python main.py --dataset cwq
```

### 主要参数说明

```bash
python main.py \
  --dataset webqsp \                    # 数据集名称 (webqsp 或 cwq)
  --device cuda:0 \                     # LLM 使用的设备
  --gnn_device cuda:0 \                 # GNN 模型使用的设备
  --topk 15 \                           # 关系检索的 top-k 数量
  --local_iter 2 \                      # 本地迭代次数
  --split test \                        # 数据集分割 (train/validation/test)
  --return_entity_threshold 20 \        # 返回实体的最大阈值
  --return_entity_min_threshold 10 \    # 返回实体的最小阈值
  --output_dir output \                 # 输出目录
```

### LLM 配置

**使用本地模型 (Gemma)**:
```bash
python main.py --dataset webqsp --llm_model_path google/gemma-2-9b-it
```

**使用 GPT API**:
```bash
python main.py --dataset webqsp \
  --is_GPT \
  --gpt_model gpt-4o-mini \
  --api_key YOUR_OPENAI_API_KEY
```

### 不确定性评估（可选）

启用不确定性评估机制：
```bash
python main.py --dataset webqsp \
  --do_uncertainty \
  --au_thres 1.55 \
  --k_au 4
```

---

## 输出结果

运行完成后，会在 `output/` 目录（或 `--output_dir` 指定的目录）下生成结果文件：

- `webqsp.jsonl` 或 `cwq.jsonl`: 包含每个问题的详细结果

结果文件格式：
```json
{
  "id": "问题ID",
  "question": "原始问题",
  "predict": ["预测答案实体列表"],
  "answer": ["标准答案实体列表"],
  "hit": 1 或 0,  # 精确匹配
  "f1": 0.85,     # F1 分数
  "precision": 0.9,
  "recall": 0.8,
  "pred_relation": ["预测使用的关系"],
  "sub_questions": ["子问题列表"]
}
```

最终会在终端显示评估结果：
```
== Eval Results ==
EM: 0.75          # 精确匹配率
F1: 0.82          # F1 分数
right: 1500       # 正确答案数量
false: 500        # 错误答案数量
#total: 2000      # 总问题数
```

---

## 项目结构说明

```
ProgRAG/
├── main.py                    # 主入口文件
├── llm.py                     # LLM 封装类
├── utils.py                   # 工具函数
├── prompts.py                 # 提示模板
├── prompt_matcher.py          # 提示匹配器
├── graph_preprocess.py        # 图数据预处理
├── finetuning_crossencoder.py # 关系检索器训练
│
├── GNN/                       # GNN 相关模块
│   ├── gnn_train.py          # GNN 训练脚本
│   ├── gnn_test.py           # GNN 测试
│   ├── gnn_utils.py          # GNN 工具函数
│   ├── gnn_text_encoder.py   # 文本编码器
│   ├── get_emb.py            # 嵌入生成
│   └── nbfmodels.py          # NBF 模型定义
│
├── MPNet/                     # MPNet 相关模块
│   ├── train.py              # MPNet 训练
│   ├── predictor.py          # MPNet 预测器
│   ├── biencoder.py          # 双编码器
│   ├── webqsp.sh             # WebQSP 训练脚本
│   └── cwq.sh                # CWQ 训练脚本
│
├── data/                      # 数据目录（需创建）
│   └── graphs/               # 图谱数据
│
└── ckpt/                      # 模型检查点目录（需创建）
    ├── GNN/                  # GNN 模型
    ├── mpnet/                # MPNet 模型
    └── sbert/                # 关系检索器模型
```

---

## 常见问题

### 1. CUDA 版本不匹配

如果遇到 CUDA 版本问题，请检查系统 CUDA 版本：
```bash
nvidia-smi
```
然后根据实际情况调整 PyTorch 和 CUDA 安装命令。

### 2. 内存不足

- 减小 `--topk` 参数
- 减小 `--return_entity_threshold`
- 使用较小的 LLM 模型

### 3. Hugging Face 模型下载慢

设置镜像或使用缓存目录：
```python
# 在代码中已设置 cache_dir='/home/huggingface'
# 可以修改为本地路径或使用镜像
```

### 4. 数据路径错误

确保数据文件路径正确：
- 图谱数据: `/data/graphs/`
- 模型检查点: `./ckpt/`

如果路径不同，需要修改 `main.py` 中相应的路径参数。

### 5. 缺少依赖包

确保所有依赖都已安装：
```bash
pip install networkx scipy openai tqdm numpy
```

---

## 论文信息

- **标题**: ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs
- **会议**: AAAI 2026
- **论文链接**: https://arxiv.org/pdf/2511.10240

---

## 许可证

请查看原项目的 LICENSE 文件。

---

## 联系方式

如有问题，请参考论文或提交 Issue。


